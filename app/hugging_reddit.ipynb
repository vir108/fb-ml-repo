{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import secrets\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models used here from HuggingFace:\n",
    "* [News Classifier](https://huggingface.co/mrm8488/bert-mini-finetuned-age_news-classification)\n",
    "* [Sentiment Analysis Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "news_tokenizer = AutoTokenizer.from_pretrained(config.HF_TOKENIZER_NEWS_CLASSIFIER)\n",
    "news_model = AutoModelForSequenceClassification.from_pretrained(config.HF_MODEL_NEWS_CLASSIFIER)\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets.REDDIT_API_CLIENT_ID,\n",
    "    client_secret=secrets.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent=secrets.REDDIT_API_USER_AGENT\n",
    ")\n",
    "subreddit = reddit.subreddit(config.NEWS_SUBREDDITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "submissions = []\n",
    "\n",
    "# Stream new submissions in from our favorite subreddits until we reach a certain number\n",
    "for submission in subreddit.stream.submissions():\n",
    "    submissions.append(submission)\n",
    "    if len(submissions) > config.NUM_SUBMISSION_TO_GET:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditSubmission():\n",
    "    subreddit: str\n",
    "    title: str\n",
    "    time_created: datetime.datetime\n",
    "    author: str\n",
    "    inference_subject: str\n",
    "    inference_sentiment: str\n",
    "\n",
    "    def __init__(self, subreddit: str, title: str, time_created: str, author: str, inference_subject: str = None, inference_sentiment: str = None):\n",
    "        self.subreddit = subreddit\n",
    "        self.title = title\n",
    "        self.time_created = self.convert_time_to_datetime(time_created)\n",
    "        self.author = author\n",
    "        self.inference_subject = self.run_subject_analysis()\n",
    "        self.inference_sentiment = self.run_sentiment_analysis()\n",
    "\n",
    "    # Convert time from Reddit API into a Python datetime object\n",
    "    def convert_time_to_datetime(self, time_created) -> datetime.datetime:\n",
    "        dt = datetime.datetime.fromtimestamp(time_created)\n",
    "        return dt\n",
    "\n",
    "    # Take the output from our news classifier and map it to a class\n",
    "    def map_news_output_to_class(self, inference_output: torch.Tensor) -> str:\n",
    "        softmax_values = []\n",
    "        for output in inference_output:\n",
    "            softmax_values.append(output.item())\n",
    "        max_value = max(softmax_values)\n",
    "        max_index = softmax_values.index(max_value)\n",
    "        return config.NEWS_CLASSES[max_index]\n",
    "    \n",
    "    # Run the news classifier model on the input\n",
    "    def run_subject_analysis(self) -> str:\n",
    "        inputs = news_tokenizer(self.title, return_tensors=\"pt\")\n",
    "        labels = torch.tensor([1]).unsqueeze(0) # Batch size of 1\n",
    "        outputs = news_model(**inputs, labels=labels) # Unpack key-value pairs into keyword args in function call\n",
    "        news_subject = self.map_news_output_to_class(outputs.logits[0]) # Taking softmax tensor from inference\n",
    "        return news_subject\n",
    "\n",
    "    # Run the sentiment analysis pipeline model on the input\n",
    "    def run_sentiment_analysis(self) -> str:\n",
    "        sentiment = sentiment_model(self.title)\n",
    "        return sentiment[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_submission_objects = []\n",
    "\n",
    "# Transform submissions into easy to handle objects\n",
    "for submission in submissions:\n",
    "    s = RedditSubmission(submission.subreddit, submission.title, submission.created_utc, submission.author)\n",
    "    reddit_submission_objects.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>time_created</th>\n",
       "      <th>author</th>\n",
       "      <th>inference_subject</th>\n",
       "      <th>inference_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>Germany: 6 men held in alleged plan to sabotag...</td>\n",
       "      <td>2022-03-31 11:44:42</td>\n",
       "      <td>stootyshooty</td>\n",
       "      <td>world</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>Ukrainians in US mobilize to help 100,000 expe...</td>\n",
       "      <td>2022-03-31 11:51:27</td>\n",
       "      <td>PhilDesenex</td>\n",
       "      <td>world</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>Deputies, EMS responding to shooting at Greenv...</td>\n",
       "      <td>2022-03-31 12:28:18</td>\n",
       "      <td>stootyshooty</td>\n",
       "      <td>business</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>UK Government ditches ban on conversion therap...</td>\n",
       "      <td>2022-03-31 12:30:05</td>\n",
       "      <td>Smilingtribute</td>\n",
       "      <td>world</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>Putin says Russia will enforce rouble payments...</td>\n",
       "      <td>2022-03-31 12:39:58</td>\n",
       "      <td>laksaleaf</td>\n",
       "      <td>business</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news</td>\n",
       "      <td>Turkish prosecutor requests transfer of Khasho...</td>\n",
       "      <td>2022-03-31 12:57:48</td>\n",
       "      <td>Ppungent</td>\n",
       "      <td>world</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>State police won't comment on audio obtained b...</td>\n",
       "      <td>2022-03-31 13:03:48</td>\n",
       "      <td>Massive_Host_7245</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news</td>\n",
       "      <td>Target will let shoppers use food stamps to pa...</td>\n",
       "      <td>2022-03-31 13:04:42</td>\n",
       "      <td>toadsns</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news</td>\n",
       "      <td>‘It’s a huge embarrassment’: Close to 200 Harr...</td>\n",
       "      <td>2022-03-31 13:16:44</td>\n",
       "      <td>zsreport</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>First complete gap-free human genome sequence ...</td>\n",
       "      <td>2022-03-31 13:19:46</td>\n",
       "      <td>Stranger1982</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news</td>\n",
       "      <td>DC Police Find 5 Fetuses in Home of Anti-Abort...</td>\n",
       "      <td>2022-03-31 13:25:30</td>\n",
       "      <td>Hrekires</td>\n",
       "      <td>world</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  \\\n",
       "0       news  Germany: 6 men held in alleged plan to sabotag...   \n",
       "1       news  Ukrainians in US mobilize to help 100,000 expe...   \n",
       "2       news  Deputies, EMS responding to shooting at Greenv...   \n",
       "3       news  UK Government ditches ban on conversion therap...   \n",
       "4       news  Putin says Russia will enforce rouble payments...   \n",
       "5       news  Turkish prosecutor requests transfer of Khasho...   \n",
       "6       news  State police won't comment on audio obtained b...   \n",
       "7       news  Target will let shoppers use food stamps to pa...   \n",
       "8       news  ‘It’s a huge embarrassment’: Close to 200 Harr...   \n",
       "9       news  First complete gap-free human genome sequence ...   \n",
       "10      news  DC Police Find 5 Fetuses in Home of Anti-Abort...   \n",
       "\n",
       "          time_created             author inference_subject  \\\n",
       "0  2022-03-31 11:44:42       stootyshooty             world   \n",
       "1  2022-03-31 11:51:27        PhilDesenex             world   \n",
       "2  2022-03-31 12:28:18       stootyshooty          business   \n",
       "3  2022-03-31 12:30:05     Smilingtribute             world   \n",
       "4  2022-03-31 12:39:58          laksaleaf          business   \n",
       "5  2022-03-31 12:57:48           Ppungent             world   \n",
       "6  2022-03-31 13:03:48  Massive_Host_7245          sci/tech   \n",
       "7  2022-03-31 13:04:42            toadsns          sci/tech   \n",
       "8  2022-03-31 13:16:44           zsreport          sci/tech   \n",
       "9  2022-03-31 13:19:46       Stranger1982          sci/tech   \n",
       "10 2022-03-31 13:25:30           Hrekires             world   \n",
       "\n",
       "   inference_sentiment  \n",
       "0             NEGATIVE  \n",
       "1             NEGATIVE  \n",
       "2             NEGATIVE  \n",
       "3             NEGATIVE  \n",
       "4             NEGATIVE  \n",
       "5             NEGATIVE  \n",
       "6             NEGATIVE  \n",
       "7             NEGATIVE  \n",
       "8             NEGATIVE  \n",
       "9             POSITIVE  \n",
       "10            NEGATIVE  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert our scraped data into a Pandas dataframe\n",
    "pd.DataFrame([vars(submission) for submission in reddit_submission_objects])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up:\n",
    "1. How do I find the top 100 posts of all time from your favorite subreddits?\n",
    "2. How do I parse comments from the post?\n",
    "3. And finally, how do I parse replies from that comment?\n",
    "4. Bonus! If you have time, browse HuggingFace and try to find an out of the box model to apply to your favorite Reddit data. Even if you can't code it up, how would you, given enough time, implement the algorithm(s)?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0fafcf4f970cab54a859adb46f4fd24f44b14105ac04ffe0bbdc9768e9518fc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('reddit-broker-bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
